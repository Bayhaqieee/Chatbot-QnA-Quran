{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnFedI8uwlOGJmeW15V0ID"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installations"
      ],
      "metadata": {
        "id": "OejZbGMs8evK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install crewai crewai-tools langchain langchain-community langchain-openai faiss-cpu pandas beautifulsoup4"
      ],
      "metadata": {
        "id": "9ExkB3vY8br0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "d01nrn7O8gz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load secrets from Colab's secret manager\n",
        "os.environ[\"AZURE_API_KEY\"] = userdata.get('AZURE_OPENAI_API_KEY')\n",
        "os.environ[\"AZURE_API_BASE\"] = userdata.get('AZURE_OPENAI_ENDPOINT')\n",
        "os.environ[\"AZURE_API_VERSION\"] = userdata.get('OPENAI_API_VERSION')\n",
        "os.environ[\"AZURE_DEPLOYMENT_ID\"] = userdata.get('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME')\n",
        "os.environ[\"AZURE_EMBEDDING_DEPLOYMENT_NAME\"] = userdata.get('AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME') # Add this line\n",
        "os.environ[\"OPENAI_API_TYPE\"] = 'azure' # Keep this to explicitly set the provider type for LiteLLM\n",
        "\n",
        "os.environ[\"SERPER_API_KEY\"] = userdata.get('SERPER_API_KEY')\n",
        "print(\"Secrets loaded successfully.\")"
      ],
      "metadata": {
        "id": "Csk_PWWDIWD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import CSVLoader"
      ],
      "metadata": {
        "id": "FpvURd-sOa16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTslzYOb8BeB"
      },
      "outputs": [],
      "source": [
        "def load_and_process_data():\n",
        "    \"\"\"Loads data from CSV files into a preliminary document format.\"\"\"\n",
        "    print(\"\\n--- Step 1: Loading and Processing Data ---\")\n",
        "    quran_docs, hadith_docs = [], []\n",
        "    try:\n",
        "        quran_loader = CSVLoader(file_path='main_df.csv', encoding='utf-8')\n",
        "        quran_docs = quran_loader.load()\n",
        "        hadith_loader = CSVLoader(file_path='all_hadiths_clean.csv', encoding='utf-8')\n",
        "        hadith_docs = hadith_loader.load()\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}. Please ensure CSV files are in the same directory.\")\n",
        "        return None, None\n",
        "    print(\"Data loading complete.\")\n",
        "    return quran_docs, hadith_docs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data and retrievers\n",
        "quran_docs, hadith_docs = load_and_process_data()\n",
        "if not (quran_docs and hadith_docs):\n",
        "    exit(\"Data loading failed. Exiting.\")"
      ],
      "metadata": {
        "id": "v8dGmV_JQnsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorized Data Storing"
      ],
      "metadata": {
        "id": "52fJKNaPCUFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "FyCB-YwYCauK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_and_vectorize(quran_docs, hadith_docs):\n",
        "    \"\"\"Converts documents into AI-readable vectors and stores them for fast retrieval.\"\"\"\n",
        "    print(\"\\n--- Step 2: Ingesting and Vectorizing Data ---\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    quran_splits = text_splitter.split_documents(quran_docs)\n",
        "    hadith_splits = text_splitter.split_documents(hadith_docs)\n",
        "\n",
        "    try:\n",
        "        embeddings = AzureOpenAIEmbeddings(\n",
        "            azure_deployment=os.environ[\"AZURE_EMBEDDING_DEPLOYMENT_NAME\"],\n",
        "            api_key=os.environ[\"AZURE_API_KEY\"],\n",
        "            azure_endpoint=os.environ[\"AZURE_API_BASE\"],\n",
        "            api_version=os.environ[\"AZURE_API_VERSION\"]\n",
        "        )\n",
        "    except KeyError as e:\n",
        "         print(f\"Azure environment variable not set: {e}.\")\n",
        "         return None, None\n",
        "\n",
        "    print(\"  - Creating FAISS vector stores...\")\n",
        "    quran_vector_store = FAISS.from_documents(documents=quran_splits, embedding=embeddings)\n",
        "    hadith_vector_store = FAISS.from_documents(documents=hadith_splits, embedding=embeddings)\n",
        "    print(\"Vector stores created.\")\n",
        "    return quran_vector_store.as_retriever(k=5), hadith_vector_store.as_retriever(k=5)"
      ],
      "metadata": {
        "id": "iVagyVtyCOyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ingest and Vectorize data\n",
        "quran_retriever, hadith_retriever = ingest_and_vectorize(quran_docs, hadith_docs)\n",
        "if not (quran_retriever and hadith_retriever):\n",
        "    exit(\"Vectorization failed. Exiting.\")"
      ],
      "metadata": {
        "id": "9C0TLd9jQv4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Tools"
      ],
      "metadata": {
        "id": "2gljGdntIkyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai_tools import BaseTool, SerperDevTool\n",
        "\n",
        "class ReligiousTextSearchTool(BaseTool):\n",
        "    name: str = \"Religious Text Search Tool\"\n",
        "    description: str = \"Searches Qur'an and Hadith vectorstores for texts relevant to a query.\"\n",
        "    quran_retriever: object\n",
        "    hadith_retriever: object\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        quran_results = self.quran_retriever.invoke(query)\n",
        "        hadith_results = self.hadith_retriever.invoke(query)\n",
        "        context = \"QURANIC SOURCES:\\n\" + \"\\n\\n\".join([doc.page_content for doc in quran_results])\n",
        "        context += \"\\n\\nHADITH SOURCES:\\n\" + \"\\n\\n\".join([doc.page_content for doc in hadith_results])\n",
        "        return context"
      ],
      "metadata": {
        "id": "aak73qvxIo1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate tools\n",
        "religious_search_tool = ReligiousTextSearchTool(\n",
        "    quran_retriever=quran_retriever,\n",
        "    hadith_retriever=hadith_retriever\n",
        ")\n",
        "serper_tool = SerperDevTool()"
      ],
      "metadata": {
        "id": "wIcBu7FQQ4a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Agents"
      ],
      "metadata": {
        "id": "iFodCXsXOt_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "\n",
        "def define_agents(llm, religious_tool, web_tool):\n",
        "    \"\"\"Defines the team of AI agents with their roles, goals, and tools.\"\"\"\n",
        "    researcher = Agent(\n",
        "        role='Primary Source Researcher',\n",
        "        goal='Find foundational texts from the Qur\\'an and Hadith relevant to the user\\'s query on {topic}.',\n",
        "        backstory='An expert in Islamic scriptures, skilled at navigating vast digital libraries of religious texts to find the most relevant passages.',\n",
        "        tools=[religious_tool], llm=llm, verbose=True\n",
        "    )\n",
        "    validator = Agent(\n",
        "        role='Contemporary Validator',\n",
        "        goal='Find contemporary views, news, and fatwas on {topic} from trusted online sources.',\n",
        "        backstory='A meticulous researcher who cross-references religious findings with modern-day discourse and scholarly opinions available on the web.',\n",
        "        tools=[web_tool], llm=llm, verbose=True\n",
        "    )\n",
        "    synthesizer = Agent(\n",
        "        role='Synthesis Agent',\n",
        "        goal='Craft a comprehensive, balanced, and well-structured answer to the user\\'s query on {topic}, integrating primary sources and contemporary views.',\n",
        "        backstory='A master communicator with deep knowledge of Islamic jurisprudence, skilled at synthesizing complex information into a clear and nuanced response.',\n",
        "        llm=llm, verbose=True\n",
        "    )\n",
        "    return researcher, validator, synthesizer"
      ],
      "metadata": {
        "id": "35w11rxLOwCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Task"
      ],
      "metadata": {
        "id": "BqfR7WLNPyim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Task\n",
        "\n",
        "def define_tasks(researcher, validator, synthesizer):\n",
        "    \"\"\"Defines the sequence of tasks for the agents to perform.\"\"\"\n",
        "    research_task = Task(\n",
        "        description='Search for primary texts (Qur\\'an and Hadith) related to the topic: {topic}.',\n",
        "        expected_output='A compiled list of relevant verses and hadiths, with full text.',\n",
        "        agent=researcher\n",
        "    )\n",
        "    validation_task = Task(\n",
        "        description='Search the web for contemporary opinions, articles, and fatwas on the topic: {topic}.',\n",
        "        expected_output='A summary of key findings from diverse and reliable online sources.',\n",
        "        agent=validator\n",
        "    )\n",
        "    synthesis_task = Task(\n",
        "        description=(\n",
        "            'Analyze the provided primary sources and contemporary web findings. '\n",
        "            'Synthesize them into a single, comprehensive answer that addresses the user\\'s query on {topic}. '\n",
        "            'The answer must be well-structured, citing different viewpoints where applicable. '\n",
        "        ),\n",
        "        expected_output='A final, curated answer that is ready to be presented to the user.',\n",
        "        agent=synthesizer,\n",
        "        context=[research_task, validation_task]\n",
        "    )\n",
        "    return [research_task, validation_task, synthesis_task]"
      ],
      "metadata": {
        "id": "IsRLILUePzqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assembling the Crew"
      ],
      "metadata": {
        "id": "l-ky1w_0P-B5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Crew, Process\n",
        "\n",
        "# Instantiate the core LLM\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_deployment=os.environ[\"AZURE_CHAT_DEPLOYMENT_NAME\"],\n",
        "    api_key=os.environ[\"AZURE_API_KEY\"],\n",
        "    azure_endpoint=os.environ[\"AZURE_API_BASE\"],\n",
        "    api_version=os.environ[\"AZURE_API_VERSION\"],\n",
        "    temperature=0.7\n",
        ")"
      ],
      "metadata": {
        "id": "8O1cebTkQA_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the agents and tasks by calling the factory functions\n",
        "agents = define_agents(llm, religious_search_tool, serper_tool)\n",
        "tasks = define_tasks(*agents)"
      ],
      "metadata": {
        "id": "jOdn3WmFQ9mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble the Crew and kickoff the process\n",
        "islamic_qna_crew = Crew(\n",
        "    agents=list(agents),\n",
        "    tasks=tasks,\n",
        "    process=Process.sequential,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "print(\"\\n Kicking off the Crew... \")\n",
        "result = islamic_qna_crew.kickoff(inputs={'topic': 'the different scholarly views on cryptocurrency'})\n",
        "\n",
        "print(\"\\n\\n FINAL CURATED RESPONSE \")\n",
        "print(\"=\"*50)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "tGhAjQgrRAUN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}